{"definitions":[{"description":"Indicates debug mode is enabled for snowflake loads.  Valid values are TRUE or FALSE.","displayName":"DEBUG_MODE","masked":false,"scopes":[11],"variableName":"SF_DEBUG_MODE"},{"description":"This is the snowflake account","displayName":"SNOWSQL_ACCOUNT","masked":false,"scopes":[11],"variableName":"SF_SNOWSQL_ACCOUNT"},{"description":"This is the snowflake database","displayName":"SNOWSQL_DATABASE","masked":false,"scopes":[11],"variableName":"SF_SNOWSQL_DATABASE"},{"description":"This is the snowflake database","displayName":"SNOWSQL_WAREHOUSE","masked":false,"scopes":[11],"variableName":"SF_SNOWSQL_WAREHOUSE"},{"description":"S3 Access Key for private bucket access","displayName":"ACCESS_KEY","masked":false,"scopes":[11],"variableName":"SF_ACCESS_KEY"},{"description":"S3 Secret Key for private bucket access","displayName":"SECRET_KEY","masked":true,"scopes":[11],"variableName":"SF_SECRET_KEY"},{"description":"The snowflake file format name for this load table\r\nThis is any value from the FILE FORMAT object type list","displayName":"FILE_FORMAT","masked":false,"scopes":[3,11,14],"variableName":"SF_FILE_FORMAT"},{"description":"Whether or not to pre-zip files","displayName":"SEND_FILES_ZIPPED","masked":false,"scopes":[11,14],"variableName":"SF_SEND_FILES_ZIPPED"},{"description":"How many rows have to exist before splitting.","displayName":"SPLIT_FILE_THRESHOLD","masked":false,"scopes":[3,11,14],"variableName":"SF_SPLIT_THRESHOLD"},{"description":"How many parts to split files into before uploading.\r\nIf not specified, defaults to 1 (not splitting)","displayName":"SPLIT_FILE_COUNT","masked":false,"scopes":[3,11,14],"variableName":"SF_SPLIT_COUNT"},{"description":"TRUE to do all database file loads as UNICODE, FALSE not to","displayName":"UNICODE_SUPPORT","masked":false,"scopes":[11,14],"variableName":"SF_UNICODE_SUPPORT"},{"description":"The type of table to create.  One of:\r\nTEMPORARY\r\nTRANSIENT\r\n{BLANK} for a normal table","displayName":"TABLE_TYPE","masked":false,"scopes":[14,15,16,17,18,19,22,23,24,25],"variableName":"SF_TABLE_TYPE"},{"description":"Specify the required timezone for any transactions\r\nNote: after any change of this property, please rerun the script:   set_timezone_parameter\r\neg: Europe/Zurich","displayName":"TIMEZONE","masked":false,"scopes":[11],"variableName":"SF_TIMEZONE"},{"description":"System maintained list of Type 3 Columns, do not edit","displayName":"TYPE_3_COLS","masked":false,"scopes":[19],"variableName":"SF_TYPE_3_COLS"},{"description":"The delimiter to use when unloading data from the legacy warehouse.\r\nDefault '|'","displayName":"UNLOAD_DELIMITER","masked":false,"scopes":[11,14],"variableName":"SF_UNLOAD_DELIMITER"},{"description":"The optionally enclosed by character to use when unloading data from the legacy warehouse.\r\nDefault a double quote.  Other valid value is a single quote.","displayName":"UNLOAD_ENCLOSED_BY","masked":false,"scopes":[11,14],"variableName":"SF_UNLOAD_ENCLOSED_BY"},{"description":"Single character string used as the escape character for any field values.\r\nUsed to automatically escape the char set in UNLOAD_ESCAPE_CHAR.\r\nWhen loading data, specifies the escape character for enclosed fields.\r\nAccepts common escape sequences up to and including ASCII 127.\r\nLeave unset to not escape.","displayName":"UNLOAD_ESCAPE_CHAR","masked":false,"scopes":[11,14],"variableName":"SF_UNLOAD_ESCAPE_CHAR"},{"description":"The load table name (a source table or source table or a source table or ...) from which incremental data is loaded","displayName":"INCREMENTAL_LOAD_TABLE","masked":false,"scopes":[14,15,19,22,24,25],"variableName":"SF_INCREMENTAL_LOAD_TABLE"},{"description":"The column name used to drive incremental loading","displayName":"INCREMENTAL_LOAD_COLUMN","masked":false,"scopes":[14,15,19,22,24,25],"variableName":"SF_INCREMENTAL_LOAD_COLUMN"},{"description":"The data type of the incremental extract column\r\nUsually one of DATE, TIMESTAMP OR INTEGER","displayName":"INCREMENTAL_LOAD_COLUMN_TYPE","masked":false,"scopes":[14,15,19,22,24,25],"variableName":"SF_INCREMENTAL_LOAD_COLUMN_TYPE"},{"description":"The RED parameter name that is updated with last loaded incremental load information\r\nNote: A value of ABC will result in two paramaters:\r\nABC_START - contains the minimum value loaded\r\nABC_END - contains the maximum value loaded","displayName":"INCREMENTAL_LOAD_PARAMETER","masked":false,"scopes":[14,15,19,22,24,25],"variableName":"SF_INCREMENTAL_LOAD_PARAMETER"},{"description":"This is the start of the file location in S3 URL syntax","displayName":"S3_BUCKET_PREFIX","masked":false,"scopes":[3],"variableName":"SF_S3_BUCKET_PREFIX"},{"description":"S3 Location to be used for external stages","displayName":"S3_LOCATION ","masked":false,"scopes":[29],"variableName":"SF_S3_LOCATION"},{"description":"The external stage to load data from","displayName":"EXTERNAL_STAGE","masked":false,"scopes":[3,14],"variableName":"SF_EXTERNAL_STAGE"},{"description":"This is the Azure Shared Access Signature used to authenticate with Azure","displayName":"AZURE_SAS_TOKEN","masked":false,"scopes":[3,14],"variableName":"SF_AZURE_SAS_TOKEN"},{"description":"This should be set to the Azure Encryption Type property on your Azure storage containers","displayName":"AZURE_ENCRYPTION_TYPE","masked":false,"scopes":[3,14],"variableName":"SF_AZURE_ENCRYPTION_TYPE"},{"description":"This should be set to the Azure Encryption Key property on your Azure storage containers","displayName":"AZURE_ENCRYPTION_KEY","masked":false,"scopes":[3,14],"variableName":"SF_AZURE_ENCRYPTION_KEY"},{"description":"The batch expression (ie: column name) used to drive a batch looping data load.","displayName":"RANGE_BATCH_EXPRESSION","masked":false,"scopes":[14],"variableName":"RANGE_BATCH_EXPRESSION"},{"description":"The batch expression (ie: column name) used to drive a batch looping data loading for post initial load incremental scenarios.","displayName":"RANGE_BATCH_INCREMENTAL_OVERRIDE","masked":false,"scopes":[14],"variableName":"RANGE_BATCH_INCREMENTAL_OVERRIDE"},{"description":"List of columns, comma seperated, that have had CR and LF chars removed from them during the extract process.","displayName":"RANGE_CR_LF_REPLACED","masked":false,"scopes":[14],"variableName":"RANGE_CR_LF_REPLACED"},{"description":"The concatination symbol for the source database, usually one of + or ||","displayName":"RANGE_CONCATWORD","masked":false,"scopes":[4,5,6,7,8,9,10,11,12,13,14],"variableName":"RANGE_CONCATWORD"},{"description":"Data export character set, one of:\r\n- {blank} - no character set statement is included in export\r\n- any valid character set name for the legacy data warehouse database","displayName":"RANGE_EXTRACT_CHARSET","masked":false,"scopes":[11,14],"variableName":"RANGE_EXTRACT_CHARSET"},{"description":"TRUE or FALSE\r\nDetermines whether a Warning or a Failure is reported after processing completes if any threads fail during batch processing.\r\n\r\nDefault FALSE","displayName":"RANGE_FAIL_ON_THREAD_FAILURE","masked":false,"scopes":[11,14],"variableName":"RANGE_FAIL_ON_THREAD_FAILURE"},{"description":"The maximum length of a row in the table","displayName":"RANGE_MAX_ROW_LENGTH","masked":false,"scopes":[14],"variableName":"RANGE_MAX_ROW_LENGTH"},{"description":"The number of thread errors to allow before stopping the task.\r\n","displayName":"RANGE_MAX_THREAD_FAILURES","masked":false,"scopes":[11],"variableName":"RANGE_MAX_THREAD_FAILURES"},{"description":"The pilot table containing the driving control data when doing a batch loop load","displayName":"RANGE_PILOT_TABLE","masked":false,"scopes":[14],"variableName":"RANGE_PILOT_TABLE"},{"description":"The maximum number of threads to use when running a batch loop table.","displayName":"RANGE_THREAD_COUNT","masked":false,"scopes":[11],"variableName":"RANGE_THREAD_COUNT"},{"description":"Indicates the extracted data files should be gzipped or not before being transferred to the cloud.\r\nValues are TRUE or FALSE\r\nNote: only used by source/target combinations where files are landed locally on the way to the cloud.","displayName":"RANGE_UNLOAD_GZIP","masked":false,"scopes":[11,14],"variableName":"RANGE_UNLOAD_GZIP"},{"description":"The number of times to retry local->cloud data file transfer if the data file transfer fails before failing the thread.","displayName":"RANGE_UPLOAD_MAX_RETRIES","masked":false,"scopes":[11],"variableName":"RANGE_UPLOAD_MAX_RETRIES"},{"description":"RED Connection pointing to SQL Server range table location.  Ignored if using SNOWFLAKE range tables.","displayName":"RANGE_WORK_CONNECTION","masked":false,"scopes":[14],"variableName":"RANGE_WORK_CONNECTION"},{"description":"Database type for range tables, one of:  SQLSERVER  SNOWFLAKE","displayName":"RANGE_WORK_TABLE_LOCATION","masked":false,"scopes":[14],"variableName":"RANGE_WORK_TABLE_LOCATION"},{"description":"Location of DBA.EXE (if not default).  Requires trailing backslash.","displayName":"SQL_ADMIN_LOCATION","masked":false,"scopes":[11],"variableName":"SQL_ADMIN_LOCATION"},{"description":"Define File Type","displayName":"FILE_TYPE","masked":false,"scopes":[3,14],"variableName":"SF_FILE_TYPE"}],"fileFormat":1}
